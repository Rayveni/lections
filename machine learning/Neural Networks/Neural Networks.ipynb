{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported numpy as np\n",
      "imported pandas as pd\n",
      "imported seaborn as sns\n",
      "imported matplotlib.pyplot as plt\n",
      "%matplotlib inline\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.MathJax_Display {text-align: left !important;}\n",
       "   .rendered_html table, .rendered_html th, .rendered_html tr, .rendered_html td {\n",
       "              border: 1px  black solid !important;\n",
       "              color: black !important;}\n",
       "            </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\Волочков\\mlopen')\n",
    "from ml_lib.jupyter import often_used_libs,jupyter_style\n",
    "np,pd,plt,sns=often_used_libs()\n",
    "jupyter_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](../../img/simple_neural_net.png)\n",
    "In neural net visuals, circles represent neurons and lines represent synapses. Synapses have a really simple job, they take a value from their input, multiply it by a specific weight, and output the result. Neurons are a little more complicated. Their job is to add together the outputs of all their synapses, and apply an activation function. Certain activation functions allow neural nets to model complex non-linear patterns, that simpler models may miss. For our neural net, we’ll use sigmoid activation functions.\n",
    "<img src='../../img/simple_neural_net.png' width=200 height=200 />\n",
    "\n",
    "|Code Symbol | Math Symbol | Definition | Dimensions\n",
    "    | :-: | :-: | :-: | :-: |\n",
    "    |X|$$X$$|Input Data, each row in an example| (numExamples, inputLayerSize)|\n",
    "    |y |$$y$$|target data|(numExamples, outputLayerSize)|\n",
    "    |W1 | $$W^{(1)}$$ | Layer 1 weights | (inputLayerSize, hiddenLayerSize) |\n",
    "    |W2 | $$W^{(2)}$$ | Layer 2 weights | (hiddenLayerSize, outputLayerSize) |\n",
    "    |z2 | $$z^{(2)}$$ | Layer 2 activation | (numExamples, hiddenLayerSize) |\n",
    "    |a2 | $$a^{(2)}$$ | Layer 2 activity | (numExamples, hiddenLayerSize) |\n",
    "    |z3 | $$z^{(3)}$$ | Layer 3 activation | (numExamples, outputLayerSize) |\n",
    "    |J | $$J$$ | Cost | (1, outputLayerSize) |\n",
    "|dJdz3 | $$\\frac{\\partial J}{\\partial z^{(3)} } = \\delta^{(3)}$$ | Partial derivative of cost with respect to $z^{(3)}$ | (numExamples,outputLayerSize)|\n",
    "    |dJdW2|$$\\frac{\\partial J}{\\partial W^{(2)}}$$|Partial derivative of cost with respect to $W^{(2)}$|(hiddenLayerSize, outputLayerSize)|\n",
    "    |dz3dz2|$$\\frac{\\partial z^{(3)}}{\\partial z^{(2)}}$$|Partial derivative of $z^{(3)}$ with respect to $z^{(2)}$|(numExamples, hiddenLayerSize)|\n",
    "    |dJdW1|$$\\frac{\\partial J}{\\partial W^{(1)}}$$|Partial derivative of cost with respect to $W^{(1)}$|(inputLayerSize, hiddenLayerSize)|\n",
    "    |delta2|$$\\delta^{(2)}$$|Backpropagating Error 2|(numExamples,hiddenLayerSize)|\n",
    "    |delta3|$$\\delta^{(3)}$$|Backpropagating Error 1|(numExamples,outputLayerSize)|\n",
    "\n",
    "Each input value, or element in matrix X, needs to be multiplied by a corresponding weight and then added together with all the other results for each neuron. This is a complex operation, but if we take the three outputs we're looking for as a single row of a matrix, and place all our individual weights into a matrix of weights, we can create the exact behavior we need by multiplying our input data matrix by our weight matrix. Using matrix multiplication allows us to pass multiple inputs through at once by simply adding rows to the matrix X. From here on out, we'll refer to these matrics as X, W one, and z two, where z two the activity of our second layer. Notice that each entry in z is a sum of weighted inputs to each hidden neuron. Z is of size 3 by 3, one row for each example, and one column for each hidden unit.\n",
    "    \n",
    "We now have our first official formula, $z^{(2)} = XW^{(1)}$. Matrix notation is really nice here, becuase it allows us to express the complex underlying process in a single line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGBy/sZ2iAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fb26c02400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Apply sigmoid activation function to scalar, vector, or matrix\n",
    "sigmoid=lambda z:1/(1+np.exp(-z))\n",
    "\n",
    "testInput = np.arange(-6,6,0.01)\n",
    "plt.plot(testInput, sigmoid(testInput), linewidth= 2)\n",
    "plt.grid(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26894142,  0.5       ,  0.73105858])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.array([-1,0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We now have our second formula for forward propagation, using f to denote our activation function, we can write that a two, our second layer activity, is equal to f of z two. a two will be a matrix of the same size as z two, 3 by 3\n",
    " \n",
    " $$\n",
    "    a^{(2)} = f(z^{(2)}) \n",
    "  $$\n",
    "  \n",
    "  To finish forward propagation we need to propagate a two all the way to the output, yhat. We've already done the heavy lifting in the previous layer, so all we have to do now is multiply a two by our senond layer weights W2 and apply one more activation funcion. W2 will be of size 3x1, one weight for each synapse. Multiplying a2, a 3 by 3, by W2, a 3 by 1 results in a 3 by 1 matrix z three, the activity or our third layer. z3 has three activity values, one for each example. Last but not least, we'll apply our activation function to z three yielding our official estimate of your test score, yHat\n",
    "  \n",
    "$$\n",
    "    \\hat{y} = f(z^{(3)}) \n",
    "    $$\n",
    "    \n",
    "We need to implement our forward propagation formulas in python. First we'll initialize our weight matrices in our init method. For starting values, we'll use random numbers.\n",
    "We'll implement forward propagation in our forward method, using numpy's built in dot method for matrix multiplication and our own sigmoid method    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propagate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to predict student's test score if we now number of hours sleeping and studying before test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = (hours sleeping, hours studying), y = Score on test\n",
    "X = np.array(([3,5], [5,1], [10,2]), dtype=float)\n",
    "y = np.array(([0.75], [0.82], [0.93]), dtype=float)\n",
    "NN=Neural_Network()\n",
    "y_hat=NN.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEZBJREFUeJzt3X9sXfV5x/H3M5MoWxMRKaEewnTOSLom1RYohlSAVlto\na0BtI1rUha5MlKQRqKk2TVRDa8Ui8U8nxMSP0UbpglIkNEvQKglVCtsfsyrE6JK0oc0PgUzaFdMW\nGndpY1hEQp794Ut2Y5z42jm+9/rr90uydM8533vO880jPpx8fe9JZCaSpLL8TqsLkCRVz3CXpAIZ\n7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFeiCVl148eLF2d3d3arLn+GNN97gPe95T6vL\nmFalz9H5zXylz7Gq+e3du/dIZl400biWhXt3dzd79uxp1eXPMDAwQG9vb6vLmFalz9H5zXylz7Gq\n+UXEfzcyzmUZSSqQ4S5JBTLcJalALVtzH8+JEycYGhri+PHjTb3uhRdeyKFDh5p6zXnz5tHV1cWc\nOXOael1Js0NbhfvQ0BALFiygu7ubiGjadY8dO8aCBQuadr3MZHh4mKGhIZYsWdK060qaPdpqWeb4\n8eMsWrSoqcHeChHBokWLmv43FEmzR1uFO1B8sL9jtsxTUmu0XbhLks5fW625j/Xxh5+t9HxPffG6\nSs8nSe2qrcNdUvuq+uZrTecI91d0Tm/kXJY5wz333MMDDzxwevvLX/4yDz74YAsrkqSpMdzr3H77\n7Tz22GMAnDp1iv7+fj772c+2uCpJmjyXZep0d3ezaNEifvjDH/Laa69xxRVXsGjRolaXJUmTZriP\nsX79erZt28Yvf/lLbr/99laXI0lT4rLMGDfddBNPP/00u3fv5qMf/Wiry5GkKWnrO/dW/MZ77ty5\n9PX1sXDhQjo6Opp+fUmqQluHeyucOnWK559/nieeeKLVpUjSlLksU+fgwYMsXbqU66+/nmXLlrW6\nHEmaMu/c66xYsYLDhw+3ugxJOm/euUtSgQx3SSqQ4S5JBTLcJalA7f0L1Z6eas+3Z8+k3/LTn/6U\nj33sY+zfv//0vk2bNjF//nzuuuuus75v+/btvP/972fFihVTKlWSzod37tNk+/btHDx4sNVlSJql\nDPc64z3yd8eOHed8zze+8Q2uuuoqVq5cyac+9SnefPNNnnvuOXbu3MmXvvQlLr/8cl5++eXpLl2S\nzmC41xnvkb/XXXcdL7/8Mpdffvnpn82bN59+zyc/+Ul2797NCy+8wPLly9m6dSvXXHMNn/jEJ7jv\nvvvYt28fl112WaumJGmWau819yY72yN/L7vsMvbt23d63KZNm06/3r9/P1/5ylc4evQoIyMjPmxM\nUlsw3MeY7CN/b7vtNrZv387KlSvZtm0bAwMD01+kJE3AZZkxJvvI32PHjnHxxRdz4sQJHn/88dP7\nFyxYwLFjx6azVEk6q/a+c5/CRxfP12Qf+XvvvfeyatUqLrroIlatWnU60NeuXcvnP/95HnroIZ58\n8knX3SU1VXuHewuMfeRvd3f3GZ9xhzPX3O+8807uvPPOd53n2muv9aOQklrGZZk6PvJXUim8c6/j\nI3+b5+MPP1vp+dZ0jnB/hedsxb8CJlWpoTv3iFgdES9GxGBE3D3O8Qsj4qmIeCEiDkTE56ZaUGZO\n9a0zymyZp6TWmDDcI6IDeAS4AVgB3BIRYx+Y8gXgYGauBHqB+yNi7mSLmTdvHsPDw8UHX2YyPDzM\nvHnzWl2KpEI1sixzNTCYmYcBIqIfWAPU/7YwgQUREcB84NfAyckW09XVxdDQEL/61a8m+9bzcvz4\n8aYH7bx58+jq6mrqNSXNHo2E+yXAK3XbQ8CqMWP+GdgJ/BxYAPxFZp6abDFz5sxhyZIlk33beRsY\nGOCKK65o+nUlabrEREsgEXEzsDoz19e2bwVWZebGMWOuBf4WuAz4d2BlZv52zLk2ABsAOjs7r+zv\n769wKlM3MjLC/PnzW13GtGq3OQ6+PlLp+RbOeZujJyb+XkKjlr63ff6soP36B+3dw3brH1TXw76+\nvr2ZOeHz0Bu5c38VuLRuu6u2r97ngK/m6P8pBiPiJ8AHgP+qH5SZW4AtAD09Pdnb29vA5affwMAA\n7VLLdGm3OVb5yRaANZ1H2fHawsrO99Sn2+vTMu3WP2jvHrZb/6D5PWzk0zK7gWURsaT2S9K1jC7B\n1PsZcD1ARHQCfwT4mUJJapEJ79wz82REbASeATqARzPzQETcUTu+GbgX2BYRPwYC+LvMPDKNdUuS\nzqGhLzFl5i5g15h9m+te/xz482pLkyRNlY8fkKQCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQWakc9z\n91ngknRu3rlLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkF\nMtwlqUCGuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDD\nXZIKZLhLUoEMd0kqkOEuSQW6oNUFSFLlenpaXcG7rVsHd901+nrPnmm/nHfuklSghu7cI2I18CDQ\nAfxLZn51nDG9wAPAHOBIZn6kwjql5mq3O78m3/Vp5psw3COiA3gE+DNgCNgdETsz82DdmIXA14DV\nmfmziHjvdBUsSZpYI8syVwODmXk4M98C+oE1Y8Z8Bvh2Zv4MIDNfr7ZMSdJkRGaee0DEzYzeka+v\nbd8KrMrMjXVj3lmO+SCwAHgwMx8b51wbgA0AnZ2dV/b390+p6MHXR6b0vrNZOOdtjp7oqOx8S987\nv7JzVWVkZIT589unrrbv4fArlZ2rCiOLFzP/yJHRjeXLW1tMTTv3sN36B9X1sK+vb29mTrhuWNWn\nZS4ArgSuB34X+M+IeD4zX6oflJlbgC0APT092dvbO6WL3f/ws+dV7FhrOo+y47WFlZ3vqU9fV9m5\nqjIwMMBU/7ynQ9v38JubKjtXFQbWraN369bRjTZZc2/nHrZb/6D5PWwk3F8FLq3b7qrtqzcEDGfm\nG8AbEfE9YCXwEpKkpmtkzX03sCwilkTEXGAtsHPMmB3AdRFxQUT8HrAKOFRtqZKkRk14556ZJyNi\nI/AMox+FfDQzD0TEHbXjmzPzUEQ8DfwIOMXoxyX3T2fhkqSza2jNPTN3AbvG7Ns8Zvs+4L7qSpMk\nTZXfUJWkAhnuklQgw12SCmS4S1KBDHdJKpDhLkkF8h/rmA7t9rhY+P9HxrbJV9clTS/v3CWpQIa7\nJBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJUIMNdkgpkuEtS\ngQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXI\ncJekAhnuklSghsI9IlZHxIsRMRgRd59j3FURcTIibq6uREnSZE0Y7hHRATwC3ACsAG6JiBVnGfeP\nwL9VXaQkaXIauXO/GhjMzMOZ+RbQD6wZZ9wXgW8Br1dYnyRpChoJ90uAV+q2h2r7TouIS4CbgK9X\nV5okaaoiM889YHT9fHVmrq9t3wqsysyNdWOeAO7PzOcjYhvwncx8cpxzbQA2AHR2dl7Z398/paIH\nXx+Z0vvOZuGctzl6oqOy8y0dfmXiQU02sngx848cgeXLW10KYA8n63T/wB42oN36B9X1sK+vb29m\n9kw07oIGzvUqcGnddldtX70eoD8iABYDN0bEyczcXj8oM7cAWwB6enqyt7e3gcu/2/0PPzul953N\nms6j7HhtYWXne+qbmyo7V1UG1q2jd+tW2LOn1aUA9nCyTvcP7GED2q1/0PweNhLuu4FlEbGE0VBf\nC3ymfkBmLnnndd2d+xnBLklqngnDPTNPRsRG4BmgA3g0Mw9ExB2145unuUZJ0iQ1cudOZu4Cdo3Z\nN26oZ+Zt51+WJOl8+A1VSSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEu\nSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJU\nIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy\n3CWpQIa7JBWooXCPiNUR8WJEDEbE3eMc/8uI+FFE/DginouIldWXKklq1IThHhEdwCPADcAK4JaI\nWDFm2E+Aj2TmHwP3AluqLlSS1LhG7tyvBgYz83BmvgX0A2vqB2Tmc5n5P7XN54GuasuUJE1GZOa5\nB0TcDKzOzPW17VuBVZm58Szj7wI+8M74Mcc2ABsAOjs7r+zv759S0YOvj0zpfWezcM7bHD3RUdn5\nlg6/Utm5qjKyeDHzjxyB5ctbXQpgDyfrdP/AHjag3foH1fWwr69vb2b2TDTugilfYRwR0QesA64b\n73hmbqG2ZNPT05O9vb1Tus79Dz87xQrHt6bzKDteW1jZ+Z765qbKzlWVgXXr6N26FfbsaXUpgD2c\nrNP9A3vYgHbrHzS/h42E+6vApXXbXbV9Z4iIPwH+BbghM4erKU+SNBWNrLnvBpZFxJKImAusBXbW\nD4iI9wHfBm7NzJeqL1OSNBkT3rln5smI2Ag8A3QAj2bmgYi4o3Z8M3APsAj4WkQAnGxkTUiSND0a\nWnPPzF3ArjH7Nte9Xg+86xeokqTW8BuqklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCX\npAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kq\nkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ\n7pJUIMNdkgpkuEtSgRoK94hYHREvRsRgRNw9zvGIiIdqx38UER+qvlRJUqMmDPeI6AAeAW4AVgC3\nRMSKMcNuAJbVfjYAX6+4TknSJDRy5341MJiZhzPzLaAfWDNmzBrgsRz1PLAwIi6uuFZJUoMaCfdL\ngFfqtodq+yY7RpLUJBc082IRsYHRZRuAkYh4sZnXP5vvwGLgSFXni6pOVKW9e0fnGG1Z3Xkrvofv\n9A+whxNryz+h6nr4B40MaiTcXwUurdvuqu2b7BgycwuwpZHCmiki9mRmT6vrmE6lz9H5zXylz7HZ\n82tkWWY3sCwilkTEXGAtsHPMmJ3AX9U+NfNh4DeZ+YuKa5UkNWjCO/fMPBkRG4FngA7g0cw8EBF3\n1I5vBnYBNwKDwJvA56avZEnSRBpac8/MXYwGeP2+zXWvE/hCtaU1VdstFU2D0ufo/Ga+0ufY1PnF\naC5Lkkri4wckqUCzKtxLf4xCA/PrjYjfRMS+2s89rahzqiLi0Yh4PSL2n+X4TO/fRPOb6f27NCL+\nIyIORsSBiPjrccbM9B42Msfm9DEzZ8UPo78Mfhn4Q2Au8AKwYsyYG4HvMvox2Q8D32913RXPrxf4\nTqtrPY85/inwIWD/WY7P2P41OL+Z3r+LgQ/VXi8AXirpv8FJzLEpfZxNd+6lP0ahkfnNaJn5PeDX\n5xgyk/vXyPxmtMz8RWb+oPb6GHCId3+Tfab3sJE5NsVsCvfSH6PQaO3X1P66+92I+GBzSmuamdy/\nRhXRv4joBq4Avj/mUDE9PMccoQl9bOrjB9RyPwDel5kjEXEjsJ3RJ3lqZiiifxExH/gW8DeZ+dtW\n1zMdJphjU/o4m+7cK3uMQpuasPbM/G1mjtRe7wLmRMTi5pU47WZy/yZUQv8iYg6jofd4Zn57nCEz\nvocTzbFZfZxN4V76YxQmnF9E/H7E6BOLIuJqRvs/3PRKp89M7t+EZnr/arVvBQ5l5j+dZdiM7mEj\nc2xWH2fNskwW/hiFBud3M3BnRJwE/hdYm7Vf388EEfGvjH7SYHFEDAH/AMyBmd8/aGh+M7p/wLXA\nrcCPI2Jfbd/fA++DMnpIY3NsSh/9hqokFWg2LctI0qxhuEtSgQx3SSqQ4S5JBTLcJalAhrskFchw\nl6QCGe6SVKD/A736yjYYQHrxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fb2a844d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Compare estimate, yHat, to actually score\n",
    "plt.bar([0,1,2], y, width = 0.35, alpha=0.8)\n",
    "plt.bar([0.35,1.35,2.35],y_hat, width = 0.35, color='r', alpha=0.8)\n",
    "plt.grid(1)\n",
    "plt.legend(['y', 'yHat']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now our predictions are pretty inaccurate. To improve our model, we first need to quantify exactly how wrong our predictions are. We'll do this with a cost function. A cost function allows us to express exactly how wrong or \"costly\" our models is, given our examples.\n",
    "\n",
    "One way to compute an overall cost is to take each error value, square it, and add these values together. Multiplying by one half will make things simpler down the road. Now that we have a cost, or job is to minimize it. When someone says they’re training a network, what they really mean is that they're minimizing a cost function.\n",
    "$$ J = \\sum \\frac{1}{2}(y-\\hat{y})^2 $$\n",
    "\n",
    "OUR cost is a function of two things, our examples, and the weights on our synapses. We don't have much control of our data, so we'll minimize our cost by changing the weights.\n",
    "\n",
    "Conceptually, this is pretty simple concept. We have a collection of 9 individual weights, and we're saying that there is some combination of w's that will make our cost, J, as small as possible. When I first saw this problem in machine learning, I thought, I'll just try ALL THE WEIGHTS UNTIL I FIND THE BEST ONE! After all I have a computer!\n",
    "\n",
    "Enter the CURSE OF DIMENSIONALITY. Here's the problem. Let's pretend for a second that we only have 1 weight, instead of 9. To find the ideal value of our weight that will minimize our cost, we need to try a bunch of values for W, let's say we test 1000 values. That doesn't seem so bad, after all, my computer is pretty fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "And since we have one big equation that uniquely determines our cost, J, from X, y, W1, and W2, we can use our good friend calculus to find what we're looking for. We want to know \"which way is downhill\", that is, what is the rate of change of J with respect to W, also known as the derivative. And in this case, since we’re just considering one weight at a time, the partial derivative.\n",
    "\n",
    "\n",
    "We can derive an expression for dJdW, that will give us the rate of change of J with respect to W, for any value of W! If dJdW is positive, then the cost function is going uphill. If dJdW is negative the cost function is going downhill.\n",
    "Now we can really speed things up. Since we know in which direction the cost decreases, we can save all that time we would have spent searching in the wrong direction. We can save even more computational time by iteratively taking steps downhill and stopping when the cost stops getting smaller.\n",
    "\n",
    "\n",
    "This method is known as gradient descent, and although it may not seem so impressive in one dimension, it is capable of incredible speedups in higher dimensions. In fact, in our final video, we’ll show that what would have taken 10^27 function evaluations with our brute force method will take less than 100 evaluations with gradient descent. Gradient descent allows us to find needles in very very very large haystacks\n",
    "\n",
    "\n",
    "Now before we celebrate too much here, there is a restriction. What if our cost function doesn't always go in the same direction? What if it goes up, then back down? The mathematical name for this is non-convex, and it could really throw off our gradient descent algorithm by getting it stuck in a local minima instead of our ideal global minima. One of the reasons we chose our cost function to be the sum of squared errors was to exploit the convex nature of quadratic equations.\n",
    "We know that the graph of y equals x squared is a nice convex parabola and it turns out that higher dimensional versions are too!\n",
    "\n",
    "\n",
    "Another piece of the puzzle here is that depending on how we use our data, it might not matter if or cost function is convex or not. If we use our examples one at a time instead of all at once, sometimes it won't matter if our cost function is convex, we will still find a good solution. This is called stochastic gradient descent.\n",
    "$$z^{(2)}=XW^{(1)}$$\n",
    "$$a^{(2)}=f(z^{(2)})$$\n",
    "$$z^{(3)}=a^{(2)}W^{(2)}$$\n",
    "$$\\hat{y} = f(z^{(3)}) $$\n",
    "$$ J = \\sum \\frac{1}{2}(y-\\hat{y})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We need to calculate :$\\frac{\\partial J}{\\partial W}$\n",
    "\n",
    "$$\n",
    "W^{(1)}=\n",
    "\\begin{bmatrix}\n",
    "    W_{11}^{(1)}       & W_{12}^{(1)} & W_{13}^{(1)} \\\\\n",
    "    W_{21}^{(1)}      & W_{22}^{(1)} & W_{23}^{(1)} \n",
    "\\end{bmatrix},\n",
    "\\frac{\\partial J}{\\partial W^{(1)}}=\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\partial J}{\\partial W_{11}^{(1)}}  & \\frac{\\partial J}{\\partial W_{12}^{(1)}} & \\frac{\\partial J}{\\partial W_{13}^{(1)}} \\\\\n",
    "        \\frac{\\partial J}{\\partial W_{21}^{(1)}} & \\frac{\\partial J}{\\partial W_{22}^{(1)}} & \\frac{\\partial J}{\\partial W_{23}^{(1)}} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "$$W^{(2)}=\n",
    "\\begin{bmatrix}\n",
    "    W_{11}^{(2)}    \\\\\n",
    "    W_{12}^{(2)}   \\\\  \n",
    "    W_{13}^{(2)}     \n",
    "\\end{bmatrix},\n",
    "\\frac{\\partial J}{\\partial W^{(2)}}=\n",
    "\\begin{bmatrix}\n",
    "   \\frac{\\partial J}{\\partial W_{11}^{(2)}}    \\\\\n",
    "    \\frac{\\partial J}{\\partial W_{12}^{(2)}}   \\\\  \n",
    "   \\frac{\\partial J}{\\partial W_{13}^{(2)}}     \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "$$\\frac{\\partial J}{\\partial W^{(2)}}= \\sum -(y-\\hat{y})\\frac{\\partial \\hat{y}}{\\partial W^{(2)}}= \\sum -(y-\\hat{y}) f^{'}(z^{(3)})  \\frac{\\partial z^{(3)}}{\\partial W^{(2)}}=\\sum -(y-\\hat{y}) f^{'}(z^{(3)}) a^{(2)}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider $-(y-\\hat{y}) f^{'}(z^{(3)})$\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "   -y_1+\\hat{y}_1   \\\\\n",
    "   -y_2+\\hat{y}_2   \\\\\n",
    "   -y_3+\\hat{y} _3    \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "   f^{'}(z_1^{(3)}) \\\\\n",
    "    f^{'}(z_2^{(3)}) \\\\\n",
    "    f^{'}(z_3^{(3)}) \n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "   \\delta_1^{(3)} \\\\\n",
    "    \\delta_2^{(3)} \\\\\n",
    "    \\delta_3^{(3)}\n",
    "\\end{bmatrix}=\\delta^{(3)} \\text {   -back propagated error}\n",
    "$$\n",
    "\n",
    "$$ a^{(2)}=\n",
    "\\begin{bmatrix}\n",
    "    a_{11}^{(2)}       & a_{12}^{(2)} & a_{13}^{(2)} \\\\\n",
    "    a_{21}^{(2)}       & a_{22}^{(2)} & a_{23}^{(2)} \\\\\n",
    "    a_{31}^{(2)}       & a_{32}^{(2)} & a_{33}^{(2)}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to multiply $\\delta^{(3)}$ and  $ a^{(2)} $ we need to transpose matrix  $ a^{(2)} $ [Scalar-by-matrix](https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-matrix)\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W^{(2)}}=\n",
    "\\begin{bmatrix}\n",
    "   \\delta_1^{(3)} \\\\\n",
    "    \\delta_2^{(3)} \\\\\n",
    "    \\delta_3^{(3)}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    a_{11}^{(2)}       & a_{12}^{(2)} & a_{13}^{(2)} \\\\\n",
    "    a_{21}^{(2)}       & a_{22}^{(2)} & a_{23}^{(2)} \\\\\n",
    "    a_{31}^{(2)}       & a_{32}^{(2)} & a_{33}^{(2)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "after transposing:\n",
    "$$\\frac{\\partial J}{\\partial W^{(2)}}=\n",
    "\\begin{bmatrix}\n",
    "    a_{11}^{(2)}       & a_{21}^{(2)} & a_{31}^{(2)} \\\\\n",
    "    a_{12}^{(2)}       & a_{22}^{(2)} & a_{32}^{(2)} \\\\\n",
    "    a_{13}^{(2)}       & a_{23}^{(2)} & a_{33}^{(2)}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "   \\delta_1^{(3)} \\\\\n",
    "    \\delta_2^{(3)} \\\\\n",
    "    \\delta_3^{(3)}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sum up\n",
    "$$\\frac{\\partial J}{\\partial W^{(2)}}=(a^{(2)})^T \\delta ^{(3)}$$\n",
    "$$\\delta ^{(3)}= -(y-\\hat{y}) f^{'}(z^{(3)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial W^{(1)}}= \\sum -(y-\\hat{y})\\frac{\\partial \\hat{y}}{\\partial W^{(1)}}= \\sum -(y-\\hat{y}) f^{'}(z^{(3)})  \\frac{\\partial z^{(3)}}{\\partial W^{(1)}}=\\sum -(y-\\hat{y}) f^{'}(z^{(3)})\\frac{\\partial a^{(2)}}{\\partial W^{(1)}}  W^{(2)}=\\sum \\delta ^{(3)} (W^{(2)})^T \\frac{\\partial a^{(2)}}{\\partial W^{(1)}} = ...  $$\n",
    "$$...=\\sum \\delta ^{(3)} (W^{(2)})^T \\frac{\\partial a^{(2)}}{\\partial W^{(1)}} =\\sum \\delta ^{(3)} (W^{(2)})^T f^{'}(z^{(2)}) \\frac{\\partial XW^{(1)}}{\\partial W^{(1)}} =\\sum \\delta ^{(3)} (W^{(2)})^T f^{'}(z^{(2)}) X^{T}$$\n",
    "$$\\delta ^{(2)}=\\sum \\delta ^{(3)} (W^{(2)})^T f^{'}(z^{(2)})$$\n",
    "So,\n",
    "$$\\frac{\\partial J}{\\partial W^{(1)}}=X^{T}\\delta ^{(2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is a good time to add a new python method for the derivative of our sigmoid function, sigmoid Prime. Our derivative should be the largest where our sigmoid function is the steepest, at the value z equals zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBAeSMJdCCE8kIS7EEJ4oP8HKbzXlAomty4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1fb2a844160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sigmoidPrime(z):\n",
    "    #Derivative of sigmoid function\n",
    "    return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "\n",
    "testValues = np.arange(-5,5,0.01)\n",
    "plt.plot(testValues, sigmoid(testValues), linewidth=2)\n",
    "plt.plot(testValues, sigmoidPrime(testValues), linewidth=2)\n",
    "plt.grid(1)\n",
    "plt.legend(['sigmoid', 'sigmoidPrime']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to think about what’s happening here is that is that each example our algorithm sees has a certain cost and a certain gradient. The gradient with respect to each example pulls our gradient descent algorithm in a certain direction. It's like every example gets a vote on which way is downhill, and when we perform batch gradient descent we just add together everyone’s vote, call it downhill, and move in that direction.\n",
    "\n",
    "\n",
    "We’ll code up our gradients in python in a new method, cost function prime. Numpy’s multiply method performs element-wise multiplication, and the dot method performs matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part of NN Class (won't work alone, needs to be included in class as \n",
    "# shown in below and in partFour.py):\n",
    "\n",
    "def costFunctionPrime(self, X, y):\n",
    "    #Compute derivative with respect to W and W2 for a given X and y:\n",
    "    self.yHat = self.forward(X)\n",
    "\n",
    "    delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "    dJdW2 = np.dot(self.a2.T, delta3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that’s left is to code this equation up in python. What’s cool here is that if we want to make a deeper neural network, we could just stack a bunch of these operations together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Whole Class with additions:\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        \n",
    "        return dJdW1, dJdW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how should we change our W’s to decrease our cost? We can now compute dJ/dW, which tells us which way is uphill in our 9 dimensional optimization space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network()\n",
    "cost1 = NN.costFunction(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08543203, -0.02136498,  0.0048385 ],\n",
       "       [ 0.05837304, -0.00538806,  0.00156592]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJdW1, dJdW2 = NN.costFunctionPrime(X,y)\n",
    "dJdW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09464871],\n",
       "       [-0.00300661],\n",
       "       [-0.00066575]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dJdW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we move this way by adding a scalar times our derivative to our weights, our cost will increase, and if we do the opposite, subtract our gradient from our weights, we will move downhill and reduce our cost. This simple step downhill is the core of gradient descent and a key part of how even very sophisticated learning algorithms are trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scalar = -0.001\n",
    "NN.W1 = NN.W1 + scalar*dJdW1\n",
    "NN.W2 = NN.W2 + scalar*dJdW2\n",
    "cost2 = NN.costFunction(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.25123412]), array([ 0.25164457]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost1, cost2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
